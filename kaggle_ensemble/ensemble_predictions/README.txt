This directory has a ensembling of the submissions of multiple kaggle entries (linked below)

Note: this is currently static, its based on the submissions for the test set, as I didn't have
time to re-implement all of the linked models to do a full ensemble training run.

For a more practical and diverse ensemble, the submissions were directly downloaded.
However the previous directory contains the code for multiple models for further experimentation

Ensembling seems to reduce overall accuracy, perhaps the solutions space is too narrowly being found
by multiple models, as no weighting of the models seems to help either.

TODO with more time: Attempt generating more data sets with the same method, to get a full
ensemble training scheme running 



Links to the following kaggle submissions:

Iter_CNN  :   https://www.kaggle.com/yakuben/crgl2020-iterative-cnn-approach/output?select=submission.csv


forward_loss_iter_CNN  :  https://www.kaggle.com/robintwhite/iterative-cnn-in-tf-keras/output


prob_extension  :   https://www.kaggle.com/yakuben/crgl-probability-extension-true-target-problem/output


quick_neighborhood  :  https://www.kaggle.com/ulrich07/quick-neighborhood-fe-mlp-keras/output


neural_CNN  :  https://www.kaggle.com/parmarsuraj99/a-neural-cnn-game-of-life-with-keras/output


Z3_contraint  :  https://www.kaggle.com/jamesmcguigan/game-of-life-z3-constraint-satisfaction/output?select=submission.csv


Iter_CNN_With_Post  :  https://www.kaggle.com/maxjeblick/crgl2020-iterative-cnn-approach-with-postproces/output?select=submission.csv


random_forest  :  https://www.kaggle.com/li325040229/the-game-of-life-reverse-with-random-forest


GAN  :  https://www.kaggle.com/seraphwedd18/application-of-gan-for-predicting-initial-state/output

